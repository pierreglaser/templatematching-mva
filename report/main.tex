\author{Pierre Glaser, Clement Chadebec}
\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage[most]{tcolorbox}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}



\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\definecolor{materialdarkbg}{RGB}{15,17,26}
\definecolor{materialdarkfg}{RGB}{143, 147, 162}
\definecolor{myblue}{rgb}{.8, .8, 1}
\definecolor{materialblue}{HTML}{82aaff}
\definecolor{materialgray}{HTML}{80869e}


\tcbset{colback=materialgray, colframe=materialblue}

\color{materialdarkfg}
\pagecolor{materialdarkbg}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
    \colorbox{white}{
        \def\svgwidth{\columnwidth}
        \import{./figures/}{#1.pdf_tex}
    }
}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}

\title{Rapport de projet - Geodesic Methods and Deformable Models}
\begin{document}
\maketitle
$ \quad $
% \pagebreak
% \part{Questions}


\paragraph{Quel est le problème traité} 
Nous étudierons dans ce rapport l'article \emph{Template Matching via densitives on the
Roto Translation Group}, par Erik J. Bekkers et. Al.

Le problème traité est la \textbf{localisation} d'un objet d'intérêt dans une image par
\textbf{apprentissage supervisé}. Dans les jeux de
données utilisés, l'objet est par exemple le disque optique d'un œil, ou bien la
rétine. Le présupposé est que les problèmes en question bénéficierait d'une prise en
compte des structures d'orientation locale en tout point de l'image.

\paragraph{Quelles sont les équations et les méthodes numériques utilisées} 
\begin{itemize}
    \item La localisation de l'objet d'intérêt se fait par \emph{cross-correlation}: un
        \emph{template} est convolué avec l'image d'intérêt. La valeur maximale du
        résultat est alors la localisation prédite de l'objet. Formellement, si $ f $
        est l'image, et $ x $
    le template:
    \[
        {x}^{\star} = \arg \max_{  } \left ( f \star t \right )(x)
    \] 
    \item Dans le cas standard, $ f $ est définie sur $ \mathbb{R}^2 $. Cependant, $ f $
        peut être transformée (relevée/soulevée?) dans $ \mathbb{R}^3 $, la dimension
        additionnelle décrivant l'état local l'orientation en tout point d'une image.
    \item Le template final est la solution d'un probleme d'optimisation de type
        moindres carrés régularisé, ou régression logistique régularisée.
        Typiquement,
        \[
        t = \arg \min_{  } \sum\limits_{ i=1 }^{ N } \left ( \langle t, p_i \rangle
        - y_i \right )^2 + R(t)
        \] 
        les $ p_i $ sont des templates individuels: ``idéaux'': ce sont des patches,
        extraits des images, de la même taille que $ t $ centrée en le point d'intérêt
        de l'image $ i $. Si l'on suppose que 
        \begin{itemize}
            \item $ \|p_i\| = 1 $
            \item n'importe quelle coupe de $ f_i $ de la taille de $ p_i $ a une norme
                de $ 1 $
        \end{itemize}
        on a $ (p_i \star f_i)(x) = \mathcal  \langle T_x(p_i) f_i[p_i] \rangle  \leq
        \|p_i\| \|f_i[p_i]\| \leq  1$, le maximum étant atteint quand $ f_i $ et $ p_i $ 
        sont alignés, ceci arrivant par construction en $ {x}^{\star}_i $
        ou $ R $ est une pénalité imposant de la régularité à t.
    \item A ce moment la, le template est une variable dans $ \mathbb{R}^N$, N étant le
        nombre de pixels du patch. On pourrait alors optimiser chaque pixel
        respectivement, mais il serait alors difficile d'imposer des contraintes de
        régularité du patch final. Une autre manière de faire est de paramétriser le
        patch comme une combinaison linéaire de fonctions régulière: c'est l'approche
        suivie dans cet article, qui utilise come fonction les B-splines. Le template
        s'ecrit alors
        \[
            t(x, y) = \sum\limits_{ k=1 }^{ N_k } \sum\limits_{ l=1 }^{ N_l } c_k B^n \left (
            \frac{x}{s_k} - k \right ) B^n \left ( \frac{y}{s_{l}} - l \right )
        \] 
        Une expression qui admet alors un gradient, ceci permettant d'ajouter au
        probleme d'optimisation un terme de la forme
        \[
            \int\limits_{  }^{  } \| \nabla_{  } t(x, y) \|^2 dx dy
        \] 
\end{itemize}

\paragraph{Pouvez vous situer l'article par rapport aux méthodes étudiées en cours et le
comparer à des sujets proches évoqués en cours} 
La différence principale avec les sujets étudiés en cours est que cette méthode est une
méthode d'apprentissage supervisée et automatique alors que les méthodes étudiées
pendant le cours sont non-supervisées et demandent généralement une intervention humaine
au moment de l'initialisation.\newline
Une caractéristique commune ce ces deux méthodes est la présence d'un terme de
pénalisation sur la régularité de la solution au problème d'optimisation, les deux
faisant intervenir le gradient le la paremétrisation: on rapelle que le problème des
contours actifs s'écrit:
\[
    \int\limits_{  \Omega }^{  } \left ( w_1^2 \|C'(s)\|^2 + w_2^2 \|C''(s)\|^2 +
    P(C(s)) \right )ds
\] 
alors que le problème étudié concerne la résolution de 
\[
    t = \arg \min_{  } \sum\limits_{ i=1 }^{ N } \left ( \langle t, p_i \rangle
    - y_i \right )^2 + \int\limits_{  }^{  } \| \nabla_{  } t(x, y) \|^2 dx dy

\] 
\paragraph{Quelle est l'originalité du travail (selon les auteurs).} 
Ce travail s'inscrit dans la continuité d'une  série d'article des auteurs: la majorité
des concepts utilisés dans cet articles on été introduits dans des publications
précédentes:
\begin{itemize}
    \item l'utilisation des scores d'orientation a été introduit en [32]
    \item l'utilisation des cake wavelets a été introduit dans la thèse de l'auteur.
    \item la cross correlation est une technique bien connue pour détecter des points
        d'intérêts.
\end{itemize}
les auteurs proposent alors une méthode de résolution pour une reformulation
``regression logistique'' permettant à cette méthode d'atteindre l'état de l'art en
termes de performances, ainsi qu'une séries d'experiences très completes
comparant l'influence des différents types de régularisation, l'importance des
orientation scores. Enfin, est donnée en appendice une interprétation mathématique d'une
version simplifiée du problème régularisé dans $ SE2(\mathbb{R}) $
\paragraph{Quels sont les résultats nouveaux importants qui en découlent.}
Le résultat le plus important ici est à nos yeux la preuve expérimentale que cette
méthode atteint l'état de l'art pour les jeux de données considérées.
\paragraph{Voyez-vous des faiblesses dans l'approche présentée et avez-vous des idées
pour y faire face?}
??
\end{document}
